{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96e3335",
   "metadata": {},
   "source": [
    "# Task 2: Recommend Items\n",
    "\n",
    "## Prompt:\n",
    "Continuing from task 1 above, if required, find dataset of customer groceries shopping basket and build a recommended extra item model. For example, provided a shopping basket containing \"pasta\" and \"olive oil\", the model may make a recommendation of \"canned tomato\" as an extra item to be added to the shopping basket.\n",
    "\n",
    "The outcome of the previous task should be useful for directly feeding into this task, you should look into reusing the output of the previous task for this task.\n",
    "\n",
    "*Please read the full documentation [[here](https://docs.google.com/document/d/1ZQARiQPf4BdPAFJjts1v5l4Ewr0ICFlaDV9t4mTUxbE/edit?usp=sharing)]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa50ad1",
   "metadata": {},
   "source": [
    "## Part 1: Preprocess Data\n",
    "\n",
    "### Step 1: Import Libaries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data1 = (pd.read_csv(\"data1.csv\"))\n",
    "\n",
    "# convert xlsx raw data file to csv\n",
    "data2_xlsx = (pd.read_excel(\"data2.xlsx\"))\n",
    "data2_xlsx.to_csv(\"data2.csv\", index=None, header=True)\n",
    "data2 = pd.DataFrame(pd.read_csv(\"data2.csv\"))\n",
    "\n",
    "data3 = (pd.read_csv(\"data3.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad3ffa",
   "metadata": {},
   "source": [
    "### Step 2: Standardize Columns & Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2551367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       order_id item_name\n",
      "0    data3_1000    Apples\n",
      "1    data3_1000    Butter\n",
      "2    data3_1000      Eggs\n",
      "3    data3_1000  Potatoes\n",
      "4    data3_1004   Oranges\n",
      "..          ...       ...\n",
      "495  data3_1493     Juice\n",
      "496  data3_1493     Bread\n",
      "497  data3_1497    Coffee\n",
      "498  data3_1497     Pasta\n",
      "499  data3_1497      Eggs\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data1_temp = data1.iloc[:, [0,5]].copy()\n",
    "data1_temp.columns = [\"order_id\", \"item_name\"]\n",
    "data1_temp['order_id'] = \"data1_\" + data1_temp['order_id'].astype(str)\n",
    "\n",
    "data2_temp = data2.iloc[:, [1,2]].copy()\n",
    "data2_temp.columns = [\"order_id\", \"item_name\"]\n",
    "data2_temp['order_id'] = \"data2_\" + data2_temp['order_id'].astype(str)\n",
    "\n",
    "data3_temp = data3.iloc[:, [0,1]].copy()\n",
    "data3_temp.columns = [\"order_id\", \"item_name\"]\n",
    "data3_temp['order_id'] = \"data3_\" + data3_temp['order_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9343b",
   "metadata": {},
   "source": [
    "### Step 3: Convert Wide to Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41b7e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         order_id                 item_name\n",
      "0         data4_0              citrus fruit\n",
      "1         data4_1            tropical fruit\n",
      "2         data4_2                whole milk\n",
      "3         data4_3                 pip fruit\n",
      "4         data4_4          other vegetables\n",
      "...           ...                       ...\n",
      "41581  data4_9792               hard cheese\n",
      "41582  data4_9796             sweet spreads\n",
      "41583  data4_9817  long life bakery product\n",
      "41584  data4_9821            red/blush wine\n",
      "41585  data4_9830                     flour\n",
      "\n",
      "[41586 rows x 2 columns]\n",
      "         order_id        item_name\n",
      "0         data5_0          burgers\n",
      "1         data5_1          chutney\n",
      "2         data5_2           turkey\n",
      "3         data5_3    mineral water\n",
      "4         data5_4   low fat yogurt\n",
      "...           ...              ...\n",
      "29253  data5_6521  frozen smoothie\n",
      "29254  data5_6593      yogurt cake\n",
      "29255  data5_6971      protein bar\n",
      "29256  data5_7179        green tea\n",
      "29257  data5_7341     tomato juice\n",
      "\n",
      "[29258 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def wide_to_long(\n",
    "    wide_data,\n",
    "    prefix='data',\n",
    "    separator='_',\n",
    "    drop_columns=None,\n",
    "    empty_val=['', 'None', None]\n",
    "  ):\n",
    "\n",
    "  \"\"\"\n",
    "  Converts dataset with wide format to long format.\n",
    "\n",
    "  Parameters:\n",
    "    wide_data: dataset input\n",
    "    prefix: the string prefix to add to order_id, to make them distinguishable\n",
    "    drop_columns: list of columns to remove\n",
    "    empty_val: values representing missing items used to remove them later\n",
    "\n",
    "  Returns:\n",
    "    new dataframe with columns ['order_id', 'item_name'] as a csv file\n",
    "  \"\"\"\n",
    "  data_temp = wide_data.copy()\n",
    "\n",
    "  if drop_columns:\n",
    "    data_temp = data_temp.drop(columns=drop_columns)\n",
    "\n",
    "  data_temp.insert(0, \"order_id\", data_temp.index)\n",
    "  data_temp = data_temp.replace(empty_val, pd.NA)\n",
    "\n",
    "  item_cols = [col for col in data_temp.columns if col.startswith('Item') or col not in ['order_id']]\n",
    "\n",
    "  data = data_temp.melt(\n",
    "    id_vars='order_id',\n",
    "    value_vars=item_cols,\n",
    "    var_name='temp',\n",
    "    value_name='item_name'\n",
    "  )\n",
    "\n",
    "  data = data.dropna(subset=['item_name'])\n",
    "  data = data.drop(columns='temp')\n",
    "  data = data[['order_id', 'item_name']].reset_index(drop=True)\n",
    "  data['order_id'] = prefix + separator + data['order_id'].astype(str)\n",
    "\n",
    "  print(data)\n",
    "\n",
    "  data.to_csv(f\"{prefix}.csv\", index=False)\n",
    "\n",
    "data4_wide = pd.read_csv(\"data4_wide.csv\")\n",
    "data4 = wide_to_long(\n",
    "  wide_data=data4_wide,\n",
    "  prefix=\"data4\",\n",
    "  drop_columns=\"Item(s)\"\n",
    ")\n",
    "data4_temp = (pd.read_csv(\"data4.csv\"))\n",
    "\n",
    "data5_wide = pd.read_csv(\"data5_wide.csv\")\n",
    "data5 = wide_to_long(\n",
    "  wide_data=data5_wide, \n",
    "  prefix=\"data5\"\n",
    ")\n",
    "data5_temp = (pd.read_csv(\"data5.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a435def",
   "metadata": {},
   "source": [
    "### Step 4: Combine Datasets & Save Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          order_id           item_name\n",
      "0       data1_1000         Wheat Flour\n",
      "1       data1_1000  Dishwashing Liquid\n",
      "2       data1_1000              Pastry\n",
      "3       data1_1000              Marker\n",
      "4       data1_1001               Saree\n",
      "...            ...                 ...\n",
      "125084  data5_6521     frozen smoothie\n",
      "125085  data5_6593         yogurt cake\n",
      "125086  data5_6971         protein bar\n",
      "125087  data5_7179           green tea\n",
      "125088  data5_7341        tomato juice\n",
      "\n",
      "[120705 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "final_data = pd.concat([data1_temp, data2_temp, data3_temp, data4_temp, data5_temp], ignore_index=True)\n",
    "\n",
    "# remove duplicates: items that appear twice in same basket\n",
    "final_data = final_data.drop_duplicates()\n",
    "\n",
    "final_data.to_csv(\"final_data.csv\", index=False)\n",
    "\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1c299",
   "metadata": {},
   "source": [
    "## Part 2: Convert Long Format to Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd06baee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basket 0: ['Wheat Flour', 'Dishwashing Liquid', 'Pastry', 'Marker']\n",
      "Basket 1: ['Saree', 'Spinach', 'Face Wash', 'Energy Drink', 'Mixer Grinder', 'Fish', 'Apple']\n",
      "Basket 2: ['Cookies', 'Chicken Breast', 'Butter', 'Dress']\n",
      "Basket 3: ['Vitamins', 'Dishwashing Liquid', 'Apple']\n",
      "Basket 4: ['Pastry', 'Salt', 'Notebook', 'Tissue']\n"
     ]
    }
   ],
   "source": [
    "transactions = (\n",
    "  final_data\n",
    "    .groupby(\"order_id\")[\"item_name\"]\n",
    "    .apply(list)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# print the first 5 baskets\n",
    "for i, basket in enumerate(transactions[:5]):\n",
    "    print(f\"Basket {i}: {basket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c94952",
   "metadata": {},
   "source": [
    "## Part 3: Run Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_assessment_coding5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
